{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb6ef129-d9d0-4090-ab93-93f5530f4f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf input.zip\n",
    "!tar -xf inference_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a28b59e1-688b-4c47-b676-84c8a23b1122",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     T5Tokenizer,\n\u001b[0;32m      4\u001b[0m     T5ForConditionalGeneration,\n\u001b[0;32m      5\u001b[0m     TrainingArguments,\n\u001b[0;32m      6\u001b[0m     Trainer\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\__init__.py:143\u001b[0m\n\u001b[0;32m    141\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    142\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 143\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    145\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "MODEL = 't5-small'\n",
    "BATCH_SIZE = 48\n",
    "NUM_PROCS = 16\n",
    "EPOCHS = 10\n",
    "OUT_DIR = 'results_t5small'\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "dataset_train = load_dataset(\n",
    "    'csv', \n",
    "    data_files='input/train.csv',\n",
    "    split='train',\n",
    "    nrows=20000\n",
    ")\n",
    "dataset_valid = load_dataset(\n",
    "    'csv', \n",
    "    data_files='input/valid.csv',\n",
    "    split='train',\n",
    "    nrows=5000\n",
    ")\n",
    "print(len(dataset_train))\n",
    "print(len(dataset_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d581de4-7105-4483-9d3c-91425669913d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(MODEL)\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"assign tag: {title} {body}\" for (title, body) in zip(examples['Title'], examples['Body'])]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    cleaned_tag = [' '.join(''.join(tag.split('<')).split('>')[:-1]) for tag in examples['Tags']]\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            cleaned_tag,\n",
    "            max_length=MAX_LENGTH,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    " \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "29066b81-a20b-4bfa-87ac-6132bed02b62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0895ac3f7a7143559fef93b8e6cb8e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb6f9ba2a06477f84ef9b2e908fd278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = dataset_train.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROCS\n",
    ")\n",
    "tokenized_valid = dataset_valid.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROCS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ed9662f-75ac-48f6-922a-76cdd93e9ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainable_params=[\n",
    "\n",
    "    'decoder.block.1.layer.2.DenseReluDense.wi.weight',\n",
    "    'decoder.block.1.layer.2.DenseReluDense.wo.weight',\n",
    "    'decoder.block.1.layer.2.layer_norm.weight',\n",
    "\n",
    "    'decoder.block.2.layer.2.DenseReluDense.wi.weight',\n",
    "    'decoder.block.2.layer.2.DenseReluDense.wo.weight',\n",
    "    'decoder.block.2.layer.2.layer_norm.weight',\n",
    "    \n",
    "    'decoder.block.3.layer.2.DenseReluDense.wi.weight',\n",
    "    'decoder.block.3.layer.2.DenseReluDense.wo.weight',\n",
    "    'decoder.block.3.layer.2.layer_norm.weight',\n",
    "   \n",
    "    'decoder.block.4.layer.2.DenseReluDense.wi.weight',\n",
    "    'decoder.block.4.layer.2.DenseReluDense.wo.weight',\n",
    "    'decoder.block.4.layer.2.layer_norm.weight',\n",
    "    \n",
    "    'decoder.block.5.layer.0.SelfAttention.q.weight',\n",
    "    'decoder.block.5.layer.0.SelfAttention.k.weight',\n",
    "    'decoder.block.5.layer.0.SelfAttention.v.weight',\n",
    "    'decoder.block.5.layer.0.SelfAttention.o.weight',\n",
    "    'decoder.block.5.layer.0.layer_norm.weight',\n",
    "    'decoder.block.5.layer.2.DenseReluDense.wi.weight',\n",
    "    'decoder.block.5.layer.2.DenseReluDense.wo.weight',\n",
    "    'decoder.block.5.layer.2.layer_norm.weight',\n",
    "    'decoder.final_layer_norm.weight'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d0f0dfcc-4ee5-498e-87f5-89e528cd6884",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda\n",
      "60,506,624 total parameters.\n",
      "11,537,920 training parameters.\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(MODEL)\n",
    "for name, param in model.named_parameters():\n",
    "    if name not in trainable_params: # choose whatever you like here\n",
    "        param.requires_grad = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device is {device}\")\n",
    "model.to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "545dc208-8245-4e9d-ba2d-7fec7732d8b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.14.344, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "training_args=TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=OUT_DIR,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=5,\n",
    "    report_to='tensorboard',\n",
    "    learning_rate=0.0001,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=4\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "68819bc2-1e48-4da3-a655-bb10789abcd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4170' max='4170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4170/4170 36:17, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.152600</td>\n",
       "      <td>0.128260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.107000</td>\n",
       "      <td>0.093065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.092500</td>\n",
       "      <td>0.082335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.085800</td>\n",
       "      <td>0.077966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.075270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.073637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.079500</td>\n",
       "      <td>0.072637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.083200</td>\n",
       "      <td>0.072171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    }
   ],
   "source": [
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc75529c-c93d-479a-abc0-48675e1e6270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "model_path = './results_t5small/checkpoint-4000'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "#tokenizer = T5Tokenizer.from_pretrained('results_t5small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cee51e18-6347-4624-a91a-157a6c0e162d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_correction(text, model, tokenizer):\n",
    "    input_text = f\"assign tag: {text}\"\n",
    "    inputs= tokenizer.encode(\n",
    "        input_text,\n",
    "        return_tensors='pt',\n",
    "        max_length=256,\n",
    "        padding='max_length',\n",
    "        truncation=True\n",
    "    )\n",
    "    corrected_ids = model.generate(\n",
    "        inputs,\n",
    "        max_length=256,\n",
    "        num_beams=5, # `num_beams=1` indicated temperature sampling.\n",
    "        early_stopping=True\n",
    "    )\n",
    "    corrected_sentence = tokenizer.decode(\n",
    "        corrected_ids[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    return corrected_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9ec23dcc-93a0-4557-b047-acd2fd2a3b08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY: Repeat Task Every Random Seconds <p>I\\'m already familiar with repeating tasks every n seconds by using Java.util.Timer and Java.util.TimerTask. But lets say I want to print \\\"Hello World\\\" to the console every random seconds from 1-5. Unfortunately I\\'m in a bit of a rush and don\\'t have any code to show so far. Any help would be apriciated\n",
      "\n",
      "TAGS: java jquery\n",
      "--------------------------------------------------------------------------------\n",
      "QUERY: I have a C++ program that I compile on Mac OS 13.4.1 using Cmake\n",
      "\n",
      "One of my users has the following error: dyld: cannot load 'my_program' (load command 0x80000034 is unknown)\n",
      "\n",
      "I have no idea why he has this error message, he is on Mac OS 10.14.6 and we both use an Intel Mac\n",
      "\n",
      "Here are some information about the binary that might be useful:\n",
      "\n",
      "otool -L my_program\n",
      "my_program:\n",
      "        /System/Library/Frameworks/OpenCL.framework/Versions/A/OpenCL (compatibility version 1.0.0, current version 1.0.0)\n",
      "        /usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 1500.65.0)\n",
      "        /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1319.100.3)\n",
      "otool -l my_program\n",
      "my_program:\n",
      "Load command 0\n",
      "      cmd LC_SEGMENT_64\n",
      "  cmdsize 72\n",
      "  segname __PAGEZERO\n",
      "   vmaddr 0x0000000000000000\n",
      "   vmsize 0x0000000100000000\n",
      "  fileoff 0\n",
      " filesize 0\n",
      "  maxprot 0x00000000\n",
      " initprot 0x00000000\n",
      "   nsects 0\n",
      "    flags 0x0\n",
      "Load command 1\n",
      "      cmd LC_SEGMENT_64\n",
      "  cmdsize 712\n",
      "  segname __TEXT\n",
      "   vmaddr 0x0000000100000000\n",
      "   vmsize 0x0000000001374000\n",
      "  fileoff 0\n",
      " filesize 20398080\n",
      "  maxprot 0x00000005\n",
      " initprot 0x00000005\n",
      "   nsects 8\n",
      "    flags 0x0\n",
      "Section\n",
      "  sectname __text\n",
      "   segname __TEXT\n",
      "      addr 0x0000000100001d00\n",
      "      size 0x00000000010ae186\n",
      "    offset 7424\n",
      "     align 2^6 (64)\n",
      "    reloff 0\n",
      "    nreloc 0\n",
      "     flags 0x80000400\n",
      " reserved1 0\n",
      " reserved2 0\n",
      "Section\n",
      "  sectname __stubs\n",
      "   segname __TEXT\n",
      "      addr 0x00000001010afe86\n",
      "      size 0x000000000000063c\n",
      "    offset 17497734\n",
      "     align 2^1 (2)\n",
      "    reloff 0\n",
      "    nreloc 0\n",
      "     flags 0x80000408\n",
      " reserved1 0 (index into indirect symbol table)\n",
      " reserved2 6 (size of stubs)\n",
      "Section\n",
      "  sectname __init_offsets\n",
      "   segname __TEXT\n",
      "      addr 0x00000001010b04c4\n",
      "      size 0x000000000000003c\n",
      "    offset 17499332\n",
      "     align 2^2 (4)\n",
      "    reloff 0\n",
      "    nreloc 0\n",
      "     flags 0x00000016\n",
      " reserved1 0\n",
      " reserved2 0\n",
      "Section\n",
      "  sectname __gcc_except_tab\n",
      "   segname __TEXT\n",
      "      addr 0x00000001010b0500\n",
      "      size 0x000000000001a69c\n",
      "    offset 17499392\n",
      "     align 2^2 (4)\n",
      "    reloff 0\n",
      "    nreloc 0\n",
      "     flags 0x00000000\n",
      " reserved1 0\n",
      " reserved2 0\n",
      "Section\n",
      "  sectname __const\n",
      "   segname __TEXT\n",
      "      addr 0x00000001010cabc0\n",
      "      size 0x000000000012f0de\n",
      "    offset 17607616\n",
      "     align 2^6 (64)\n",
      "    reloff 0\n",
      "    nreloc 0\n",
      "     flags 0x00000000\n",
      " reserved1 0\n",
      " reserved2 0\n",
      "Section\n",
      "  sectname __cstring\n",
      "   segname __TEXT\n",
      "      addr 0x00000001011f9ca0\n",
      "      size 0x00000000000ac6f8\n",
      "    offset 18848928\n",
      "     align 2^4 (16)\n",
      "    reloff 0\n",
      "    nreloc 0\n",
      "     flags 0x00000002\n",
      " reserved1 0\n",
      " reserved2 0\n",
      "Section\n",
      "  sectname __unwind_info\n",
      "   segname __TEXT\n",
      "      addr 0x00000001012a6398\n",
      "      size 0x0000000000014ef8\n",
      "    offset 19555224\n",
      "     align 2^2 (4)\n",
      "    reloff 0\n",
      "    nreloc 0\n",
      "     flags 0x00000000\n",
      " reserved1 0\n",
      " reserved2 0\n",
      "Section\n",
      "  sectname __eh_frame\n",
      "   segname __TEXT\n",
      "      addr 0x00000001012bb290\n",
      "      size 0x00000000000b8d38\n",
      "    offset 19640976\n",
      "     align 2^3 (8)\n",
      "    reloff 0\n",
      "    nreloc 0\n",
      "     flags 0x00000000\n",
      " reserved1 0\n",
      " reserved2 0\n",
      "Load command 2\n",
      "      cmd LC_SEGMENT_64\n",
      "  cmdsize 232\n",
      "  segname __DATA_CONST\n",
      "   vmaddr 0x0000000101374000\n",
      "   vmsize 0x000000000001c000\n",
      "  fileoff 20398080\n",
      " filesize 114688\n",
      "  maxprot 0x00000003\n",
      " initprot 0x00000003\n",
      "   nsects 2\n",
      "    flags 0x10\n",
      "Section\n",
      "  sectname __got\n",
      "   segname __DATA_CONST\n",
      "      addr 0x0000000101374000\n",
      "      size 0x0000000000000d78\n",
      "    offset 20398080\n",
      "     align 2^3 (8)\n",
      "    reloff 0\n",
      "    nreloc 0\n",
      "     flags 0x00000006\n",
      " reserved1 266 (index into indirect symbol table)\n",
      " reserved2 0\n",
      "Section\n",
      "  sectname __const\n",
      "   segname __DATA_CONST\n",
      "      addr 0x0000000101374d80\n",
      "      size 0x0000000000017b58\n",
      "    offset 20401536\n",
      "     align 2^4 (16)\n",
      "    reloff 0\n",
      "    nreloc 0\n",
      "     flags 0x00000000\n",
      " reserved1 0\n",
      " reserved2 0\n",
      "Load command 3\n",
      "      cmd LC_SEGMENT_64\n",
      "  cmdsize 312\n",
      "  segname __DATA\n",
      "   vmaddr 0x0000000101390000\n",
      "   vmsize 0x0000000000034000\n",
      "  fileoff 20512768\n",
      " filesize 81920\n",
      "  maxprot 0x00000003\n",
      " initprot 0x00000003\n",
      "   nsects 3\n",
      "    flags 0x0\n",
      "Section\n",
      "  sectname __data\n",
      "   segname __DATA\n",
      "      addr 0x0000000101390000\n",
      "      size 0x0000000000011050\n",
      "    offset 20512768\n",
      "     align 2^6 (64)\n",
      "    reloff 0\n",
      "    nreloc 0\n",
      "     flags 0x00000000\n",
      " reserved1 0\n",
      " reserved2 0\n",
      "Section\n",
      "  sectname __bss\n",
      "   segname __DATA\n",
      "      addr 0x00000001013a1080\n",
      "      size 0x000000000001ec48\n",
      "    offset 0\n",
      "     align 2^6 (64)\n",
      "    reloff 0\n",
      "    nreloc 0\n",
      "     flags 0x00000001\n",
      " reserved1 0\n",
      " reserved2 0\n",
      "Section\n",
      "  sectname __common\n",
      "   segname __DATA\n",
      "      addr 0x00000001013bfcd0\n",
      "      size 0x00000000000007f0\n",
      "    offset 0\n",
      "     align 2^4 (16)\n",
      "    reloff 0\n",
      "    nreloc 0\n",
      "     flags 0x00000001\n",
      " reserved1 0\n",
      " reserved2 0\n",
      "Load command 4\n",
      "      cmd LC_SEGMENT_64\n",
      "  cmdsize 72\n",
      "  segname __LINKEDIT\n",
      "   vmaddr 0x00000001013c4000\n",
      "   vmsize 0x00000000001b4000\n",
      "  fileoff 20594688\n",
      " filesize 1772472\n",
      "  maxprot 0x00000001\n",
      " initprot 0x00000001\n",
      "   nsects 0\n",
      "    flags 0x0\n",
      "Load command 5\n",
      "      cmd LC_DYLD_CHAINED_FIXUPS\n",
      "  cmdsize 16\n",
      "  dataoff 20594688\n",
      " datasize 25456\n",
      "Load command 6\n",
      "      cmd LC_DYLD_EXPORTS_TRIE\n",
      "  cmdsize 16\n",
      "  dataoff 20620144\n",
      " datasize 177624\n",
      "Load command 7\n",
      "     cmd LC_SYMTAB\n",
      " cmdsize 24\n",
      "  symoff 20829768\n",
      "   nsyms 29929\n",
      "  stroff 21311424\n",
      " strsize 1055736\n",
      "Load command 8\n",
      "            cmd LC_DYSYMTAB\n",
      "        cmdsize 80\n",
      "      ilocalsym 0\n",
      "      nlocalsym 21530\n",
      "     iextdefsym 21530\n",
      "     nextdefsym 7957\n",
      "      iundefsym 29487\n",
      "      nundefsym 442\n",
      "         tocoff 0\n",
      "           ntoc 0\n",
      "      modtaboff 0\n",
      "        nmodtab 0\n",
      "   extrefsymoff 0\n",
      "    nextrefsyms 0\n",
      " indirectsymoff 21308632\n",
      "  nindirectsyms 697\n",
      "      extreloff 0\n",
      "        nextrel 0\n",
      "      locreloff 0\n",
      "        nlocrel 0\n",
      "Load command 9\n",
      "          cmd LC_LOAD_DYLINKER\n",
      "      cmdsize 32\n",
      "         name /usr/lib/dyld (offset 12)\n",
      "Load command 10\n",
      "     cmd LC_UUID\n",
      " cmdsize 24\n",
      "    uuid 482C4C59-09D1-358F-85F5-B6D96F0D359F\n",
      "Load command 11\n",
      "      cmd LC_BUILD_VERSION\n",
      "  cmdsize 32\n",
      " platform 1\n",
      "    minos 13.0\n",
      "      sdk 13.3\n",
      "   ntools 1\n",
      "     tool 3\n",
      "  version 857.1\n",
      "Load command 12\n",
      "      cmd LC_SOURCE_VERSION\n",
      "  cmdsize 16\n",
      "  version 0.0\n",
      "Load command 13\n",
      "       cmd LC_MAIN\n",
      "   cmdsize 24\n",
      "  entryoff 7424\n",
      " stacksize 0\n",
      "Load command 14\n",
      "          cmd LC_LOAD_DYLIB\n",
      "      cmdsize 88\n",
      "         name /System/Library/Frameworks/OpenCL.framework/Versions/A/OpenCL (offset 24)\n",
      "   time stamp 2 Wed Dec 31 20:00:02 1969\n",
      "      current version 1.0.0\n",
      "compatibility version 1.0.0\n",
      "Load command 15\n",
      "          cmd LC_LOAD_DYLIB\n",
      "      cmdsize 48\n",
      "         name /usr/lib/libc++.1.dylib (offset 24)\n",
      "   time stamp 2 Wed Dec 31 20:00:02 1969\n",
      "      current version 1500.65.0\n",
      "compatibility version 1.0.0\n",
      "Load command 16\n",
      "          cmd LC_LOAD_DYLIB\n",
      "      cmdsize 56\n",
      "         name /usr/lib/libSystem.B.dylib (offset 24)\n",
      "   time stamp 2 Wed Dec 31 20:00:02 1969\n",
      "      current version 1319.100.3\n",
      "compatibility version 1.0.0\n",
      "Load command 17\n",
      "      cmd LC_FUNCTION_STARTS\n",
      "  cmdsize 16\n",
      "  dataoff 20797768\n",
      " datasize 30104\n",
      "Load command 18\n",
      "      cmd LC_DATA_IN_CODE\n",
      "  cmdsize 16\n",
      "  dataoff 20827872\n",
      " datasize 1896\n",
      "\n",
      "TAGS: c++\n",
      "--------------------------------------------------------------------------------\n",
      "QUERY: C++: `error: expected ',' or '...' before string constant` before `__FILE__` in `fancy_abort (__FILE__, __LINE__, __FUNCTION__)` when compiling gcc\n",
      "This is a continuation of trying to compile a license-free GPL version of the Microchip XC32 microcontroller g++ XC32 v4.35 cross-compiler from source.\n",
      "\n",
      "See my Q&A here, and my repo here: https://github.com/ElectricRCAircraftGuy/Microchip_XC32_Compiler.\n",
      "\n",
      "Note that this builds perfectly to completion on Linux Ubuntu 22.04 with gcc --version gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0.\n",
      "\n",
      "However, when compiling on Windows in the MSYS2 UCRT64 environment (set up per my instructions here) with gcc --version gcc.exe (Rev2, Built by MSYS2 project) 13.2.0, I now get the following error in gcc/gcc/system.h:\n",
      "\n",
      "./../../pic32m-source/gcc/gcc/prefix.c\n",
      "../../../pic32m-source/gcc/gcc/system.h:737:30: error: expected identifier before string constant\n",
      "  737 | #define abort() fancy_abort (__FILE__, __LINE__, __FUNCTION__)\n",
      "      |                              ^~~~~~~~\n",
      "../../../pic32m-source/gcc/gcc/system.h:737:30: error: expected ',' or '...' before string constant\n",
      "../../../pic32m-source/gcc/gcc/system.h:737:30: error: expected identifier before string constant\n",
      "  737 | #define abort() fancy_abort (__FILE__, __LINE__, __FUNCTION__)\n",
      "      |                              ^~~~~~~~\n",
      "../../../pic32m-source/gcc/gcc/system.h:737:30: error: expected ',' or '...' before string constant\n",
      "make[1]: *** [Makefile:1112: prefix.o] Error 1\n",
      "make[1]: Leaving directory '/c/Users/gabriel/GS/dev/Microchip_XC32_Compiler/xc32-v4.35-src/pic32m-build/gcc/gcc'\n",
      "make: *** [Makefile:4290: all-gcc] Error 2\n",
      "Here is the system.h file I am using, at line 737:\n",
      "\n",
      "/* Redefine abort to report an internal error w/o coredump, and\n",
      "   reporting the location of the error in the source file.  */\n",
      "extern void fancy_abort (const char *, int, const char *)\n",
      "                     ATTRIBUTE_NORETURN ATTRIBUTE_COLD;\n",
      "#define abort() fancy_abort (__FILE__, __LINE__, __FUNCTION__)\n",
      "This definition of abort() is getting called, for instance, inside a C++ template class in fibonacci_heap.h line 460, here:\n",
      "\n",
      "template<class K, class V>\n",
      "V*\n",
      "fibonacci_heap<K,V>::delete_node (fibonacci_node<K,V> *node, bool release)\n",
      "{\n",
      "  V *ret = node->m_data;\n",
      "\n",
      "  /* To perform delete, we just make it the min key, and extract.  */\n",
      "  replace_key (node, m_global_min_key);\n",
      "  if (node != m_min)\n",
      "    {\n",
      "      fprintf (stderr, \"Can't force minimum on fibheap.\\n\");\n",
      "      abort ();  // <=========== HERE ===========\n",
      "    }\n",
      "  extract_min (release);\n",
      "\n",
      "  return ret;\n",
      "}\n",
      "How do I resolve this build error?\n",
      "\n",
      "And, why is it occurring on Windows, but not on Linux?\n",
      "\n",
      "Additional research:\n",
      "Useful Google search: msys gcc abort fancy_abort error: expected identifier before string constant\n",
      "\n",
      "Possible bug: https://sourceforge.net/p/mingw-w64/bugs/974/\n",
      "\n",
      "Possible fix: https://github.com/yosshin4004/xdev68k/blob/main/build_m68k-toolchain.sh#L199-L210\n",
      "\n",
      "#\n",
      "#   æ–°ã—ã„ mingw ç’°å¢ƒã§ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªã‚¨ãƒ©ãƒ¼ã¨ãªã‚‹ã€‚\n",
      "#       ../../../src/gcc-10.2.0/gcc/system.h:743:30: error: expected identifier before string constant\n",
      "#       743 | #define abort() fancy_abort (__FILE__, __LINE__, __FUNCTION__)\n",
      "#   å¿œæ€¥å‡¦ç½®ã¨ã—ã¦ã€å•é¡Œã‚’èµ·ã“ã™è¡Œã‚’é™¤åŽ»ã™ã‚‹ã€‚\n",
      "#   abort() ã¯ stdlib.h å†…ã§å®£è¨€ã•ã‚ŒãŸå®Ÿè£…ã®ã¾ã¾ã®æŒ™å‹•ã¨ãªã‚‹ã€‚\n",
      "#\n",
      "if [ \"$(expr substr $(uname -s) 1 5)\" == \"MINGW\" ]; then\n",
      "    cat ${SRC_DIR}/${GCC_DIR}/gcc/system.h |\\\n",
      "    perl -e 'my $before=\"#define abort() fancy_abort (__FILE__, __LINE__, __FUNCTION__)\";my $after=\"/* $before */\";$before=quotemeta($before);while(<>){$_=~s/$before/$after/g;print $_;}' > ${SRC_DIR}/${GCC_DIR}/gcc/system.h.tmp;\n",
      "    mv ${SRC_DIR}/${GCC_DIR}/gcc/system.h.tmp ${SRC_DIR}/${GCC_DIR}/gcc/system.h\n",
      "fi\n",
      "Translation of the comments from Japanese to English:\n",
      "\n",
      "#\n",
      "# In the new mingw environment, the following error will occur.\n",
      "# ../../../src/gcc-10.2.0/gcc/system.h:743:30: error: expected identifier before string constant\n",
      "# 743 | #define abort() fancy_abort (__FILE__, __LINE__, __FUNCTION__)\n",
      "# As a workaround, remove the offending line.\n",
      "# abort() behaves as the implementation declared in stdlib.h.\n",
      "\n",
      "TAGS: c++ microchip\n",
      "--------------------------------------------------------------------------------\n",
      "QUERY: How do I change the grid size with Matplotlib?\n",
      "This is what I want:\n",
      "\n",
      "properly spaced grid\n",
      "\n",
      "This is what I currently have:\n",
      "\n",
      "my grid\n",
      "\n",
      "This is my code so far:\n",
      "\n",
      "l = [-1, 1, -10, 10]\n",
      "plt.axis(l)\n",
      "plt.grid()\n",
      "\n",
      "TAGS: javascript matplotlib\n",
      "--------------------------------------------------------------------------------\n",
      "QUERY: I can't SVG file open in react.js . Svg File Include CSS And Html file separate\n",
      "i downloaded SVG Files from Stroyset, this website svg files like below link Here is the SVG Link when you click Export button you get three option like this Image if i click download button i download svg format, this file can open in local browser i can't in react app why?\n",
      "\n",
      "when going like with image tag it showing error. who can i open this file in react.js\n",
      "\n",
      "TAGS: javascript reactjs reactjs reactjs reactjs\n",
      "--------------------------------------------------------------------------------\n",
      "QUERY: I would like to know how to reindex Express program (.CDX) files using PHP\n",
      "Currently, I can insert data using the xbase library in PHP, but I'm facing an issue where I have to manually reindex the data in the Express program after inserting it. I would like to know how to write a PHP program that can trigger a reindexing process after inserting data. Here is the code I'm using for the insert\n",
      "\n",
      "$remote_path8 = 'GLJNLIT.DBF';\n",
      "  $temp_file = tempnam(sys_get_temp_dir(), 'dbf_temp');\n",
      "  $connection = ftp_connect($ftp_server);\n",
      "  ftp_login($connection, $ftp_user, $ftp_pass);\n",
      "  if (ftp_get($connection, $temp_file, $remote_path8, FTP_BINARY)){\n",
      "\n",
      "  $table1 = new TableEditor(\n",
      "    $temp_file,\n",
      "    [\n",
      "        'editMode' => TableEditor::EDIT_MODE_CLONE,\n",
      "    ]\n",
      "  );\n",
      "     $record1 = $table1->appendRecord();\n",
      "     $record->set('VOUCHER', '111111');\n",
      "     $record1->set('Seqit', '3');\n",
      "     $record1->set('Voudat', '20231107');\n",
      "     $record1->set('Accnum', '1154-00');\n",
      "     $record1->set('Depcod', '');\n",
      "     $record1->set('Jobcod', '');\n",
      "     $record1->set('Phase', '');\n",
      "     $record1->set('Coscod', '');\n",
      "     $record1->set('Descrp', 'JIB');\n",
      "     $record1->set('Trntyp', '0');\n",
      "     $record1->set('Amount', '1');\n",
      "     $record1->set('Chgdat', '20231107');\n",
      "     $record1->set('Chgtim', '0951');\n",
      "     $record1->set('Adjust', '');\n",
      "     $record1->set('Chgaccfrom', '');\n",
      "\n",
      "     $table1\n",
      "     ->writeRecord($record1)\n",
      "     ->save()\n",
      "     ->close();\n",
      "\n",
      "      ftp_put($connection, $remote_path8, $temp_file, FTP_BINARY); \n",
      "      ftp_close($connection);\n",
      "      unlink($temp_file);\n",
      "\n",
      "    echo \"<br> DBF file GLJNL<br>\";\n",
      "  }else{\n",
      "    echo \"<br>Error downloading DBF file GLJNL\";\n",
      "  }\n",
      "\n",
      "TAGS: php xbase\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: 'inference_data/.ipynb_checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_89/2549273814.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inference_data/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"inference_data/{file}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcorrected_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_correction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'inference_data/.ipynb_checkpoints'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for file in os.listdir('inference_data/'):\n",
    "    f = open(f\"inference_data/{file}\", 'r')\n",
    "    sentence = f.read()\n",
    "    corrected_sentence = do_correction(sentence, model, tokenizer)\n",
    "    print(f\"QUERY: {sentence}\\nTAGS: {corrected_sentence}\")\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07288047-b81f-4b8f-afae-8a4a972f20d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
